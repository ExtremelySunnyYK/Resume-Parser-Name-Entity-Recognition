{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../resume_parser/')\n",
    "import resume_parser\n",
    "# from resume_parser import resume_parser\n",
    "\n",
    "jd_path = \"../data/JD/Software Engineer Intern in ML Engineering Platform (System) - 2021 Summer - ByteDance.pdf\"\n",
    "resume_path = \"../data/resume/ivan_machine_learning_engineer.pdf\"\n",
    "job_description = resume_parser.get_text(jd_path)\n",
    "resume = resume_parser.get_text(resume_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chang_Ker_Fui.docx',\n",
       " 'Chang_Ker_Fui_Resume.pdf',\n",
       " 'ivan_machine_learning_engineer.pdf',\n",
       " 'Jaryl Lim Resume.docx',\n",
       " 'machine_learning_engineer_airbnb.pdf',\n",
       " 'resumes.zip',\n",
       " 'Ruolin_zhang_psych.pdf',\n",
       " 'sample_input.pdf',\n",
       " 'sample_output.txt',\n",
       " 'testdata.json',\n",
       " 'traindata.json',\n",
       " 'train_data.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../data/resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A list of text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[0-9]', '',text).replace(u'\\xa0', u' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "resume,job_description = clean_text(resume),clean_text(job_description)\n",
    "text = [resume, job_description]\n",
    "\n",
    "\n",
    "predefined_stop_words = [\"the\",\"a\",\"is\",\"you\",\"your\",\"will\"]\n",
    "\n",
    "stop_words = list(set(stopwords.words('english'))) + predefined_stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline method with cosine similarity and Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/resume/ivan_machine_learning_engineer.pdf\n",
      "\n",
      "Similarity Scores Lemma and stopwords:\n",
      "[[1.       0.625759]\n",
      " [0.625759 1.      ]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/Yk/OneDrive - Singapore University of Technology and Design/Startup/MatchedIn/Code/resume_parser/env/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:386: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'doe', 'ha', 'might', 'must', \"n't\", 'need', 'sha', 'wa', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# A list of text\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "stop_words = list(set(stopwords.words('english'))) + predefined_stop_words\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "cv = CountVectorizer(analyzer=clean_text,tokenizer=LemmaTokenizer(),stop_words=stop_words)\n",
    "\n",
    "\n",
    "# Lemma and stopwords\n",
    "print(resume_path)\n",
    "cv = CountVectorizer(tokenizer=LemmaTokenizer(),stop_words=stop_words)\n",
    "# generates word counts for the words in resume and jd \n",
    "count_matrix = cv.fit_transform(text)\n",
    "#Print the similarity scores\n",
    "print(\"\\nSimilarity Scores Lemma and stopwords:\")\n",
    "print(cosine_similarity(count_matrix))\n",
    "\n",
    "# # Lemma without stop words\n",
    "# cv = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "# count_matrix = cv.fit_transform(text)\n",
    "# #Print the similarity scores\n",
    "# print(\"\\nSimilarity Scores Lemma without stop words:\")\n",
    "# print(cosine_similarity(count_matrix))\n",
    "\n",
    "# # Stop words without lemma\n",
    "# cv = CountVectorizer(stop_words=stop_words)\n",
    "# count_matrix = cv.fit_transform(text)\n",
    "# #Print the similarity scores\n",
    "# print(\"\\nSimilarity Scores Lemma without stop words:\")\n",
    "# print(cosine_similarity(count_matrix))\n",
    "\n",
    "# # without lemma and stop words\n",
    "# cv = CountVectorizer()\n",
    "# count_matrix = cv.fit_transform(text)\n",
    "# #Print the similarity scores\n",
    "# print(\"\\nSimilarity Scores without lemma and stop words:\")\n",
    "# print(cosine_similarity(count_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = count_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                        columns=cv.get_feature_names(), \n",
    "                        index=['resume', 'job_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "# tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "# tfidf_transformer.fit(count_matrix)\n",
    "# # print idf values\n",
    "# df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# # sort ascending \n",
    "# df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# # count matrix \n",
    "# count_vector=cv.transform(resume) \n",
    " \n",
    "# # tf-idf scores \n",
    "# tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_candidate = resume_parser.extract_skills(resume)\n",
    "skills_jd = resume_parser.extract_skills(job_description)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Health',\n",
       " 'Accounting',\n",
       " 'Consulting',\n",
       " 'Segmentation',\n",
       " 'Algorithms',\n",
       " 'Negotiation',\n",
       " 'Spark',\n",
       " 'Networking',\n",
       " 'Cloud',\n",
       " 'Business process',\n",
       " 'Research',\n",
       " 'International',\n",
       " 'Marketing',\n",
       " 'Salesforce',\n",
       " 'Process',\n",
       " 'Plan',\n",
       " 'Ui',\n",
       " 'Metrics',\n",
       " 'Crm',\n",
       " 'Ai',\n",
       " 'Usability',\n",
       " 'Analysis',\n",
       " 'Cola',\n",
       " 'Sap',\n",
       " 'Front-end',\n",
       " 'Recruit',\n",
       " 'Data collection',\n",
       " 'Tensorflow',\n",
       " 'Visual',\n",
       " 'Matrix',\n",
       " 'Distribution',\n",
       " 'Schedule',\n",
       " 'Database',\n",
       " 'Business intelligence',\n",
       " 'Sql',\n",
       " 'Training',\n",
       " 'Communication',\n",
       " 'Mining',\n",
       " 'Sas',\n",
       " 'Design',\n",
       " 'Operations',\n",
       " 'Computer science',\n",
       " 'Sales',\n",
       " 'Ux',\n",
       " 'Hadoop',\n",
       " 'Programming',\n",
       " 'Analytics',\n",
       " 'System',\n",
       " 'Engineering']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skills_candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Raw Skill Matching by vectorizing both jd and skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Scores.\n",
      "[[1.         0.30123762]\n",
      " [0.30123762 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "skills = [' '.join(skills_jd),' '.join(skills_candidate)]\n",
    "cv = CountVectorizer()\n",
    "skills_matrix = cv.fit_transform(skills)\n",
    "#Print the similarity scores\n",
    "print(\"\\nSimilarity Scores.\")\n",
    "print(cosine_similarity(skills_matrix))\n",
    "threshold = 0.2\n",
    "\n",
    "# this one is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Matching by training vectorizer first. Finding the distance of the resume from the job parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Process Process\n",
      "Tensorflow Tensorflow\n",
      "Training Training\n",
      "Communication Communication\n",
      "Computer science Computer science\n",
      "Programming Programming\n",
      "System System\n",
      "Engineering Engineering\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "skills_candidate[9:12]\n",
    "skills_jd[10]\n",
    "\n",
    "count = 0\n",
    "for i in skills_candidate:\n",
    "    for j in skills_jd:\n",
    "        if i ==j:\n",
    "            count+=1\n",
    "            print(i,j)\n",
    "percentage_match = count/len(skills_jd)\n",
    "print(percentage_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The cosine similarity for the resume is 0.662.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0   1   2   3   4   5   6   7   8   9   10  11  12  13  14  15  16  17  18\n",
       "0   0   0   1   1   0   0   0   1   0   0   2   1   0   0   0   1   1   1   1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def vect_cos(vect, resume_list):\n",
    "    \"\"\" Vectorise text and compute the cosine similarity \"\"\"\n",
    "    jd = vect.transform([' '.join(vect.get_feature_names())])\n",
    "    # print(\"jd\"+str(jd))\n",
    "    resume_vect = vect.transform(resume_list)\n",
    "    # print(\"resume\"+str(resume_vect))\n",
    "    cos_sim = cosine_similarity(jd.A, resume_vect.A)  # displays the resulting matrix\n",
    "    cos_sim = cosine_similarity(resume_vect.A,jd.A)  # displays the resulting matrix\n",
    "\n",
    "    return resume_vect, np.round(cos_sim.squeeze(), 3)\n",
    "\n",
    "# vectoriser = CountVectorizer().fit(skills_jd)\n",
    "vectoriser = CountVectorizer().fit(skills_candidate)\n",
    "\n",
    "\n",
    "# Analyze candidate\n",
    "resume_vect, resume_cos = vect_cos(vectoriser, [' '.join(skills_jd)])\n",
    "print('\\nThe cosine similarity for the resume is {}.'.format(resume_cos))\n",
    "\n",
    "# turning it into df\n",
    "pd.DataFrame(resume_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 : Using Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cosine similarity for the resume is 0.25555062599997597.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count word occurrences\n",
    "resume_vals = Counter(skills_candidate)\n",
    "jd_vals = Counter(skills_jd)\n",
    "\n",
    "# convert to word-vectors\n",
    "words  = list(resume_vals.keys() | jd_vals.keys())\n",
    "resume_vect = [resume_vals.get(word, 0) for word in words]\n",
    "jd_vect = [jd_vals.get(word, 0) for word in words]        # [1, 1, 1, 0, 1, 0]\n",
    "\n",
    "# find cosine\n",
    "len_r  = sum(rv*rv for rv in resume_vect) ** 0.5             # sqrt(7)\n",
    "len_jd  = sum(jdv*jdv for jdv in jd_vect) ** 0.5             # sqrt(4)\n",
    "dot    = sum(rv*jdv for rv,jdv in zip(resume_vect, jd_vect))    # 3\n",
    "cosine = dot / (len_r * len_jd)\n",
    "print('The cosine similarity for the resume is {}.'.format(cosine))\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 4 : Jaacard Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The jaccard similarity for the resume is 0.13114754098360656.\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.metrics import jaccard_score Not useful\n",
    "\n",
    "def jaccard_similarity(list1, list2):\n",
    "    intersection = len(list(set(list1).intersection(list2)))\n",
    "    union = (len(list1) + len(list2)) - intersection\n",
    "    return float(intersection) / union\n",
    "\n",
    "j_sim = jaccard_similarity(skills_candidate,skills_jd)\n",
    "print('The jaccard similarity for the resume is {}.'.format(j_sim))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
