{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import re\n",
    "import sys\n",
    "sys.path.append('../resume_parser/')\n",
    "import resume_parser\n",
    "# from resume_parser import resume_parser\n",
    "\n",
    "jd_path = \"../data/JD/Software Engineer Intern in ML Engineering Platform (System) - 2021 Summer - ByteDance.pdf\"\n",
    "resume_path = \"../data/resume/ivan_machine_learning_engineer.pdf\"\n",
    "job_description = resume_parser.get_text(jd_path)\n",
    "resume = resume_parser.get_text(resume_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Chang_Ker_Fui.docx',\n",
       " 'Chang_Ker_Fui_Resume.pdf',\n",
       " 'ivan_machine_learning_engineer.pdf',\n",
       " 'Jaryl Lim Resume.docx',\n",
       " 'machine_learning_engineer_airbnb.pdf',\n",
       " 'resumes.zip',\n",
       " 'Ruolin_zhang_psych.pdf',\n",
       " 'sample_input.pdf',\n",
       " 'sample_output.txt',\n",
       " 'testdata.json',\n",
       " 'traindata.json',\n",
       " 'train_data.pkl']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir(\"../data/resume\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# A list of text\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'[0-9]', '',text).replace(u'\\xa0', u' ')\n",
    "    return text\n",
    "\n",
    "\n",
    "resume,job_description = clean_text(resume),clean_text(job_description)\n",
    "text = [resume, job_description]\n",
    "\n",
    "\n",
    "predefined_stop_words = [\"the\",\"a\",\"is\",\"you\",\"your\",\"will\"]\n",
    "\n",
    "stop_words = list(set(stopwords.words('english'))) + predefined_stop_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline method with cosine similarity and Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/yk09/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "../data/resume/ivan_machine_learning_engineer.pdf\n",
      "\n",
      "Similarity Scores Lemma and stopwords:\n",
      "[[1.       0.625759]\n",
      " [0.625759 1.      ]]\n",
      "\n",
      "Similarity Scores Lemma without stop words:\n",
      "[[1.         0.69737129]\n",
      " [0.69737129 1.        ]]\n",
      "\n",
      "Similarity Scores Lemma without stop words:\n",
      "[[1.         0.26869837]\n",
      " [0.26869837 1.        ]]\n",
      "\n",
      "Similarity Scores without lemma and stop words:\n",
      "[[1.         0.66345316]\n",
      " [0.66345316 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# A list of text\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize          \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "stop_words = list(set(stopwords.words('english'))) + predefined_stop_words\n",
    "\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]\n",
    "\n",
    "cv = CountVectorizer(analyzer=clean_text,tokenizer=LemmaTokenizer(),stop_words=stop_words)\n",
    "\n",
    "\n",
    "# Lemma and stopwords\n",
    "print(resume_path)\n",
    "cv = CountVectorizer(tokenizer=LemmaTokenizer(),stop_words=stop_words)\n",
    "# generates word counts for the words in resume and jd \n",
    "count_matrix = cv.fit_transform(text)\n",
    "#Print the similarity scores\n",
    "print(\"\\nSimilarity Scores Lemma and stopwords:\")\n",
    "print(cosine_similarity(count_matrix))\n",
    "\n",
    "# # Lemma without stop words\n",
    "# cv = CountVectorizer(tokenizer=LemmaTokenizer())\n",
    "# count_matrix = cv.fit_transform(text)\n",
    "# #Print the similarity scores\n",
    "# print(\"\\nSimilarity Scores Lemma without stop words:\")\n",
    "# print(cosine_similarity(count_matrix))\n",
    "\n",
    "# # Stop words without lemma\n",
    "# cv = CountVectorizer(stop_words=stop_words)\n",
    "# count_matrix = cv.fit_transform(text)\n",
    "# #Print the similarity scores\n",
    "# print(\"\\nSimilarity Scores Lemma without stop words:\")\n",
    "# print(cosine_similarity(count_matrix))\n",
    "\n",
    "# # without lemma and stop words\n",
    "# cv = CountVectorizer()\n",
    "# count_matrix = cv.fit_transform(text)\n",
    "# #Print the similarity scores\n",
    "# print(\"\\nSimilarity Scores without lemma and stop words:\")\n",
    "# print(cosine_similarity(count_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix = count_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                        columns=cv.get_feature_names(), \n",
    "                        index=['resume', 'job_description'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer,TfidfTransformer\n",
    "# tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "# tfidf_transformer.fit(count_matrix)\n",
    "# # print idf values\n",
    "# df_idf = pd.DataFrame(tfidf_transformer.idf_, index=cv.get_feature_names(),columns=[\"idf_weights\"]) \n",
    " \n",
    "# # sort ascending \n",
    "# df_idf.sort_values(by=['idf_weights'])\n",
    "\n",
    "# # count matrix \n",
    "# count_vector=cv.transform(resume) \n",
    " \n",
    "# # tf-idf scores \n",
    "# tf_idf_vector=tfidf_transformer.transform(count_vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Skill Based Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "skills_candidate = resume_parser.extract_skills(resume)\n",
    "skills_jd = resume_parser.extract_skills(job_description)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 1 : Raw Skill Matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Similarity Scores without lemma and stop words:\n",
      "[[1.         0.30123762]\n",
      " [0.30123762 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "skills = [' '.join(skills_jd),' '.join(skills_candidate)]\n",
    "cv = CountVectorizer()\n",
    "skills_matrix = cv.fit_transform(skills)\n",
    "#Print the similarity scores\n",
    "print(\"\\nSimilarity Scores without lemma and stop words:\")\n",
    "print(cosine_similarity(skills_matrix))\n",
    "threshold = 0.2\n",
    "\n",
    "# this one is lower"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 2 : Matching by training vectorizer first. Finding the distance of the resume from the job parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System System\n",
      "Communication Communication\n",
      "Tensorflow Tensorflow\n",
      "Computer science Computer science\n",
      "Engineering Engineering\n",
      "Training Training\n",
      "Process Process\n",
      "Programming Programming\n",
      "0.4\n"
     ]
    }
   ],
   "source": [
    "skills_candidate[9:12]\n",
    "skills_jd[10]\n",
    "\n",
    "count = 0\n",
    "for i in skills_candidate:\n",
    "    for j in skills_jd:\n",
    "        if i ==j:\n",
    "            count+=1\n",
    "            print(i,j)\n",
    "percentage_match = count/len(skills_jd)\n",
    "print(percentage_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The cosine similarity for the resume is 0.662.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "get_feature_names not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-67d8df25eef5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# turning it into df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mresume_vect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/mnt/c/Users/Yk/OneDrive - Singapore University of Technology and Design/Startup/MatchedIn/Code/resume_parser/env/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    685\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 687\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    689\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: get_feature_names not found"
     ]
    }
   ],
   "source": [
    "def vect_cos(vect, resume_list):\n",
    "    \"\"\" Vectorise text and compute the cosine similarity \"\"\"\n",
    "    jd = vect.transform([' '.join(vect.get_feature_names())])\n",
    "    # print(\"jd\"+str(jd))\n",
    "    resume_vect = vect.transform(resume_list)\n",
    "    # print(\"resume\"+str(resume_vect))\n",
    "    cos_sim = cosine_similarity(jd.A, resume_vect.A)  # displays the resulting matrix\n",
    "    return resume_vect, np.round(cos_sim.squeeze(), 3)\n",
    "\n",
    "vectoriser = CountVectorizer().fit(skills_jd)\n",
    "\n",
    "# Analyze candidate\n",
    "resume_vect, resume_cos = vect_cos(vectoriser, [' '.join(skills_candidate)])\n",
    "print('\\nThe cosine similarity for the resume is {}.'.format(resume_cos))\n",
    "\n",
    "# turning it into df\n",
    "pd.DataFrame(resume_vect.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method 3 : Using Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1] [1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'len_a' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-6854c72affed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0mlen_jd\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdv\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjdv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mjdv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mjd_vect\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m0.5\u001b[0m             \u001b[0;31m# sqrt(4)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdot\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mjdv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mjdv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresume_vect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjd_vect\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m    \u001b[0;31m# 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mcosine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdot\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen_a\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mlen_b\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'The cosine similarity for the resume is {}.'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'len_a' is not defined"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# count word occurrences\n",
    "resume_vals = Counter(skills_candidate)\n",
    "jd_vals = Counter(skills_jd)\n",
    "\n",
    "# convert to word-vectors\n",
    "words  = list(resume_vals.keys() | jd_vals.keys())\n",
    "resume_vect = [resume_vals.get(word, 0) for word in words]\n",
    "jd_vect = [jd_vals.get(word, 0) for word in words]        # [1, 1, 1, 0, 1, 0]\n",
    "\n",
    "# find cosine\n",
    "len_r  = sum(rv*rv for rv in resume_vect) ** 0.5             # sqrt(7)\n",
    "len_jd  = sum(jdv*jdv for jdv in jd_vect) ** 0.5             # sqrt(4)\n",
    "dot    = sum(rv*jdv for rv,jdv in zip(resume_vect, jd_vect))    # 3\n",
    "cosine = dot / (len_r * len_jd)\n",
    "print('The cosine similarity for the resume is {}.'.format(cosine))\n",
    "                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching Education"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
